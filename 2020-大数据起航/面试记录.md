# 0. JVM 基础

JVM三大组成部分：

![image-20200910160821598](C:\Users\hds\AppData\Roaming\Typora\typora-user-images\image-20200910160821598.png)

其中：方法区(JDK8改为元空间) 是直接内存。

**紫色线程独享，黄色线程共享。**

##  0.1 类装载子系统

类装载步骤

双亲委派机制

类加载器类别

如何绕开双亲委派机制

##  0.2 运行时数据区  5部分

###  0.2.1 栈(线程)  栈帧 4部分 ==可以指向堆内存==

​	https://blog.csdn.net/qq_40121580/article/details/107441214?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-6-107441214.nonecase&utm_term=%E6%96%B9%E6%B3%95%E5%87%BA%E5%8F%A3%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%20%E6%A0%88

​	每个线程对应一块独立的栈内存空间。

​	线程中的每个方法又单独对应一块独立的栈内存空间===栈帧，存方法的局部变量

​	![image-20200910162502789](C:\Users\hds\AppData\Roaming\Typora\typora-user-images\image-20200910162502789.png)                                                                                                                           

栈帧的内部结构：==javap -c 对代码进行反汇编==    还有 jad.exe 工具

```
.\jad.exe -sjava .\EnumSingle.class
```

>    局部变量表   ==存储局部变量==
>
>    操作数栈	   ==计算过程中数据的临时中转内存区域==
>
>    动态链接		==比较抽象，稍后讲解==
>
>    方法出口		

代码在栈帧中的执行过程看这里：

https://www.bilibili.com/video/BV1g741187nB?p=3  诸葛 或者

https://www.bilibili.com/video/BV1jT4y1u74D?p=12 13 14  昭君

**方法出口**

无论方法采用何种方式退出，在方法退出后都需要返回到方法被调用的位置，程序才能继续执行，方法返回时可能需要在当前栈帧中保存一些信息，用来帮他恢复它的上层方法执行状态。

方法退出过程实际上就等同于把当前栈帧出栈，因此退出可以执行的操作有：恢复上层方法的局部变量表和操作数栈，把返回值(如果有的话)压如调用者的操作数栈中，调整PC计数器的值以指向方法调用指令后的下一条指令。

一般来说，方法正常退出时，==调用者的PC计数值可以作为返回地址==，栈帧中可能保存此计数值。而方法异常退出时，返回地址是通过异常处理器表确定的，栈帧中一般不会保存此部分信息。

### 0.2.2 本地方法栈

​	本地方法：native  底层是 系统函数库中用 C 实现的方法。

​	这些方法中需要的内存空间就是本地方法栈。

### 0.2.3 程序计数器

​	也是每一个线程独享，记录每一个线程程序运行的位置，

​	以便线程切换的时候可以定位到需要继续执行的位置。

**特点**

- 线程私有
- JVM规范中唯一没有规定OutOfMemoryError情况的区域
- 如果正在执行的是Native 方法，则这个计数器值为空

**首先，为什么是线程私有？**
Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现，也就是说，在同一时刻一个处理器内核只会执行一条线程，处理器切换线程时并不会记录上一个线程执行到哪个位置，所以为了线程切换后依然能恢复到原位，每条线程都需要有各自独立的程序计数器。

**为什么没有规定OutOfMemoryError？**
如上文，程序计数器存储的是字节码文件的行号，而这个范围是可知晓的，在一开始分配内存时就可以分配一个绝对不会溢出的内存。

**为什么执行Native方法，值为空？**
Native方法大多是通过C实现并未编译成需要执行的字节码指令，也就不需要去存储字节码文件的行号了。

### 0.2.4 堆

>  新生代 Eden Survivor（From To）

>  老年代 Old

==没有永久代了==

GCRoot 有哪些？ 

1.虚拟机栈：栈帧中的本地变量表引用的对象

2.native方法引用的对象

3.方法区中的静态变量和常量引用的对象

https://www.bilibili.com/video/BV1g741187nB?p=5  没看完 回家继续

### 0.2.5 方法区(元空间)  ==可以指向堆内存==

​	这块区域是直接内存 ???

​	普通常量也在方法区  ???

​	常量 + 静态变量 + 类信息

​	字符串常量池和运行时常量池究竟去了哪里？

​	https://www.cnblogs.com/cosmos-wong/p/12925299.html

​	在JDK1.8中，使用元空间代替永久代来实现方法区，但是方法区并没有改变，

​	所谓"Your father will 	always be your father"，变动的只是方法区中内容的物理存放位置。

​	正如上面所说，类型信息（元数据信息）等其他信息被移动到了元空间中；

​	但是运行时常量池和字符串常量池被移动到了堆中。

​	但是不论它们物理上如何存放，逻辑上还是属于方法区的。

​	JDK1.8中字符串常量池和运行时常量池逻辑上属于方法区，

​	但是实际存放在堆内存中，因此既可以说两者存放在堆中，

​	也可以说两则存在于方法区中，这就是造成误解的地方。

## 0.3  字节码执行引擎

负责字节码的执行，并在程序执行过程中动态修改程序计数器的值。

#  1. JAVA 基础

https://www.jianshu.com/p/49e76079243d

## 1.1AQS 原理  ==未看==

## 1.2 多态什么意思

https://blog.csdn.net/qq_42937522/article/details/106563188?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param

对于重载而言，`在方法调用之前，编译器就已经确定了所要调用的方法`，这称为“早绑定”或“静态绑定” ；
而`对于多态，只有等到方法调用的那一刻， 解释运行器才会确定所要调用的具体方法，这称为“晚绑定”或“动态绑定”` 。
引用一句Bruce Eckel的话： “`不要犯傻，如果它不是晚绑定， 它就不是多态。`”
所以说，`方法的重载并不是多态性的一种体现`。

### 如何理解多态性？

可以理解为一个事物的多种形态。

### 何为多态性？

对象的多态性：父类的引用指向子类的对象（或子类的对象赋给父类的引用）

- 可以直接应用在抽象类和接口上

### 多态性的作用

提高了代码的通用性，常称作接口重用

### 多态性的使用

**虚拟方法调用**

有了对象的多态性以后，我们在编译期，只能调用父类中声明的方法，但在运行期，我们实际执行的是子类重写父类的方法。

总结：**编译，看左边；运行，看右边。**

### 多态性的注意事项

Java引用变量有两个类型： 编译时类型和运行时类型。 编译时类型由声明该变量时使用的类型决定， 运行时类型由实际赋给该变量的对象决定。 简称： 编译时， 看左边；运行时， 看右边。

- 若编译时类型和运行时类型不一致， 就出现了对象的多态性(Polymorphism)

- 多态情况下， “看左边” ： 看的是父类的引用（父类中不具备子类特有的方法）；“看右边” ： 看的是子类的对象（实际运行的是子类重写父类的方法）

- 对象的多态 —在Java中,子类的对象可以替代父类的对象使用

  - 一个变量只能有一种确定的数据类型

  - 一个引用类型变量可能指向(引用)多种不同类型的对象

    ```java
    Person p = new Student();
    Object o = new Person();//Object类型的变量o， 指向Person类型的对象
    o = new Student(); //Object类型的变量o， 指向Student类型的对象
    123
    ```

- 子类可看做是特殊的父类， 所以父类类型的引用可以指向子类的对象：向上转型(upcasting)。

- 一个引用类型变量如果声明为父类的类型，但实际引用的是子类对象，那么该变量就不能再访问子类中添加的属性和方法

  ```java
  Student m = new Student();
  m.school = “pku”; //合法,Student类有school成员变量
  Person e = new Student();
  e.school = “pku”; //非法,Person类没有school成员变量
  1234
  ```

  属性是在编译时确定的，编译时e为Person类型，没有school成员变量，因而编译错误。

- ==`对象的多态性，只适用于方法，不适用于属性（编译和运行都看左边）`==

### 多态性的使用前提

① 类的继承或者实现关系 ② 方法的重写 ③向上转型

### 多态性示例

方法声明的形参类型为父类类型，可以使用子类的对象作为实参调用该方法

```java
public class Test {
    public void method(Person e) {
        // ……
        e.getInfo();
    }
    public static void main(Stirng args[]) {
        Test t = new Test();
        Student m = new Student();
        t.method(m); // 子类的对象m传送给父类类型的参数e
    }
}
1234567891011
```

### 虚拟方法调用(Virtual Method Invocation)

#### 正常的方法调用

```java
Person e = new Person();
e.getInfo();
Student e = new Student();
e.getInfo();
1234
```

#### 虚拟方法调用(多态情况下)

子类中定义了与父类同名同参数的方法，在多态情况下，将此时父类的方法称为虚拟方法，父类根据赋给它的不同子类对象，动态调用属于子类的该方法。这样的方法调用在编译期是无法确定的。

```java
Person e = new Student();
e.getInfo(); //调用Student类的getInfo()方法
12
```

#### 编译时类型和运行时类型

编译时e为Person类型，而方法的调用是在运行时确定的，所以调用的是Student类的getInfo()方法。 ——动态绑定

#### 图示

![image-20200601084747489](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20va29hbGEwMTAvdHlwb3JhL3Jhdy9tYXN0ZXIvaW1nL2ltYWdlLTIwMjAwNTMwMjE1NzU2MDQ4LnBuZw?x-oss-process=image/format,png)

##### 前提

Person类中定义了welcome()方法，各个子类重写了welcome()。

##### 执行：

多态的情况下，调用对象的welcome()方法，实际执行的是子类重写的方法。

## 1.3 final、volate、synchronized关键字

## 1.4 单例模式



##  1.5  String 

  intern() 方法：

##  1.6 JAVA 对象头 Epoch 是什么？

![image-20200910200159850](C:\Users\kasa\AppData\Roaming\Typora\typora-user-images\image-20200910200159850.png)

# 2. Hive

## 2.1 如何注册 UDF，如何永久生效

注册UDF 分为临时注册和永久注册，参考资料

[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateFunction](https://links.jianshu.com/go?to=https%3A%2F%2Fcwiki.apache.org%2Fconfluence%2Fdisplay%2FHive%2FLanguageManual%2BDDL%23LanguageManualDDL-CreateFunction)

###### 1、 临时函数

临时函数以session生命周期(比如你打开关闭 hive cli 或者beeline)
 示例代码（在shell里）

```csharp
add jar /home/hadoop/zgh/hiveudf-1.0-SNAPSHOT.jar;
create temporary function ceshi as 'com.example.udf.DataMaxUdf';
```

这样即可注册一个临时函数了。

###### 2、永久注册

将其打包上传到hdfs 的目录下,在hive  cli 中执行

```csharp
create function zgh.data_max as 'com.example.udf.DataMaxUdf' using jar 'hdfs:///user/zgh/hiveudf.jar';
```

这样就在zgh databases  下面创建了一个 data_max的自定义函数。当每次访问函数时，他会默认去hdfs 加载当前函数。

注意

1、比如传入的参数需要时bigInt格式的话，这边就需要传入Long 类型的格式,，比如这种，Integer类型是不支持bigInt的

```kotlin
public class DataMinUdf extends UDF {

    public Long evaluate(Long a, Long b) {
        if (a == null || b==null) { return null; }
        return a>b?b:a;
    }
}
```

2、hiverserver2  访问的时候发现注册的udf 访问不到

hive在1.2版本以后，使用jdbc 访问hiveserver2 和hive cli 将有可能访问不到udf。官网解释如下

```bash
creating permanent functions in one Hive CLI session may not be reflected in HiveServer2 or other Hive CLI sessions,
 if they were started before the function was created. 
Issuing RELOAD FUNCTIONS within a HiveServer2 or HiveCLI session will allow it to pick up any changes to the permanent functions that may have been done by a different HiveCLI session.
Due to backward compatibility reasons RELOAD FUNCTION; is also accepted.
```

所以，在访问udf的时候，需要先执行

```undefined
RELOAD FUNCTIONS
```

来获取udf的更新情况。

## 2.2 如何设置 mapper 和 reducer 个数

==原则：我们要在数据处理的map数和单map处理的记录数上做到平衡。==

https://www.iteye.com/blog/hugh-wangp-1579192

http://www.linuxidc.com/Linux/2014-04/99726.htm

可以计算估计、直接指定、还可如下处理

补充一种方法，工作中遇到的，而且已经作为我们的最终方案。

`set dfs.block.size`

不管是32MB,64MB,128MB或者256MB，只要保证一个block处理的记录数是自身环境所适应的就OK了

## 2.3 order by,sort by,distribute by,cluster by 区别

### 总说：

笼统地看，这四个在hive中都有排序和聚集的作用，然而，它们在执行时所启动的MR却各不相同。

### 细讲：

**order by：**

order by会对所给的全部数据进行全局排序，并且只会“叫醒”一个reducer干活。

它就像一个糊涂蛋一样，不管来多少数据，都  ==***只启动一个reducer来处理***==。

因此，数据量小还可以，但数据量一旦变大order by就会变得异常吃力，甚至“罢工”。

**sort by：**

sort by是局部排序。相比order by的懒惰糊涂，sort by正好相反，它不但非常勤快，而且具备分身功能。

sort by会根据数据量的大小启动一到多个reducer来干活，并且，它会在进入reduce之前为每个reducer都产生一个排序文件。

这样的好处是提高了全局排序的效率。

**distribute by：**

distribute by的功能是：distribute by 控制map结果的分发，它会将具有相同字段的map输出分发到一个reduce节点上做处理。

即就是，某种情况下，我们需要控制某个特定行到某个reducer中，这种操作一般是为后续可能发生的聚集操作做准备。

举一个最常见的栗子：

![img](https://img-blog.csdn.net/20180829222310675?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzk1MjE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

 接上面，

![img](https://img-blog.csdn.net/20180829222337803?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzk1MjE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

 

 

以上栗子为在根据年份和气温对气象数据进行排序时，我们希望看到同一年的数据被放到同一个reducer中去处理。

==因而，这个结果也肯定是全局排序的。==

==特别的，因为distribute by 通常和sort by 一起用，所以当distribute by 遇上 sort by时，distribute by要放在前面，==

这个不难理解，因为要先通过distribute by 将待处理的数据从map端做分发，这样，sort by 这个擅长局部排序的才能去放开的干活。

不然要是没有distribute by的分发，那么sort by 将要处理全部的数据，即全局排序，这不是sort by的活，这样做只能拖慢集群工作效率。

**cluster by：**

cluster by，在《Hadoop权威指南第二版》中这样描述道：

![img](https://img-blog.csdn.net/20180829220849762?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNzk1MjE0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

 也就是说，如果参照上面气象数据的栗子，当二者皆取year列时，sql语句如下：

```sql
from recrds2 select year , temperature cluster by year;
```

## 2.4 Hive SQL 如何转化成MR任务的

Hive将SQL转化为MapReduce任务整个编译阶段分为六个阶段：

1. Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree
2. 遍历AST Tree，抽象出查询的基本组成单元QueryBlock
3. 遍历QueryBlock，翻译为执行操作树OperatorTree
4. 逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量
5. 遍历OperatorTree，翻译为MapReduce任务
6. 物理层优化器进行MapReduce任务的变换，生成最终的执行计划

## 2.5 怎么验证Hive SQL 的正确性

没理解：语法的正确性还是结果的正确性？？

## 2.6 lateral view explode关键字来拆分数组

横向视图与用户定义的表生成函数（如explode（））结合使用。

 如内置表生成函数中所述，UDTF为每个输入行生成零个或多个输出行。 

横向视图首先将UDTF应用于基表的每一行，然后将结果输出行连接到输入行，以形成具有所提供的表别名的虚拟表。

https://zhuanlan.zhihu.com/p/115913870

https://www.deeplearn.me/2892.html

## 2.7 join操作底层的MapReduce是怎么去执行的

这篇博客特别好：

``````java
https://blog.csdn.net/qq_36153312/article/details/105387892?utm_medium=distribute.pc_relevant_right.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param_right&depth_1-utm_source=distribute.pc_relevant_right.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param_right
``````

## 2.8 ORC parquet 存储格式

## 2.9 Hive 压缩

## 2.10 高级聚合函数

https://www.deeplearn.me/3866.html

https://blog.csdn.net/HappyRocking/article/details/106545559

`grouping sets`  `grouping__id(两个下划线_)`

## 2.11 窗口函数

# 3. Hadoop

## 3.1 Hadoop Shuffle和Spark Shuffle的区别

https://blog.csdn.net/qq_36381640/article/details/84016829

## 3.2 MapReduce程序中的经过几次排序

![image-20200910102508350](C:\Users\hds\AppData\Roaming\Typora\typora-user-images\image-20200910102508350.png)

在Map任务和Reduce任务的过程中，一共发生了3次排序

1）当map函数产生输出时，会首先写入内存的环形缓冲区，当达到设定的阀值，在刷写磁盘之前，

​      后台线程会将缓冲区的数据划分成相应的分区。在每个分区中，后台线程按键进行内排序

2）在Map任务完成之前，磁盘上存在多个已经分好区，并排好序的，大小和缓冲区一样的溢写文件，

​      这时溢写文件将被合并成一个已分区且已排序的输出文件。由于溢写文件已经经过第一次排序，

​      所有合并文件只需要再做一次排序即可使输出文件整体有序。

3）在reduce阶段，需要将多个Map任务的输出文件copy到ReduceTask中后合并，由于经过第二次排序，

​      所以合并文件时只需再做一次排序即可使输出文件整体有序

**总结：**在这3次排序中第一次是内存缓冲区做的内排序，使用的算法是快速排序，

​           第二次排序和第三次排序都是在文件合并阶段发生的，使用的是归并排序。

## 3.3 解决 web 页面操作无权限的问题：

Hadoop 默认开启权限检查，使用 dir.who 作为 http 访问的 静态用户，

因此可以通过关闭权限检查或者配置 http 静态用户为所需用户即可。

在 core-site.xml 中修改 http 访问的静态用户为 Kasa

```shell
<property>
	<name>hadoop.http.staticuser.user</name>
	<value>Kasa</value>
</property>
```

在 hdfs-site.xml 中关闭权限检查

```java
<property>
	<name>dfs.permissions.enabled</name>
	<value>false</value>
</property>
```

##  3.4  [Hadoop集群添加新节点步骤](https://www.cnblogs.com/yaohaitao/p/5792327.html)

1.在新节点中进行操作系统配置,包括主机名。网络。防火墙和无密码登录等。

2.在所有节点/etc/host文件中添加新节点

3.把namenode的有关配置文件复制到该节点

4.修改master节点slaves文件,增加改节点

5.单独启动该节点上的datanode和nodemanager

$hadoop-daemon.sh start datanode(在新增加节点启动 datanode)

$yarn-daemon.sh start nodemanager

运行start-balancer.sh 进行数据负载均衡

（默认阀值:单个节点的使用率和整个集群的使用率差值是10%以下,超过百分之十集群就会进行负载均衡）

#  4. 数据结构和算法 

https://www.cs.usfca.edu/~galles/visualization/RedBlack.html

##  AVL 树

https://blog.csdn.net/yuyang_z/article/details/88393494

##  红黑树

https://www.bilibili.com/video/BV1tE411f7tP?p=3

红黑树的性质：具有二叉查找树的特点。

1、节点颜色非黑即红；

2、根节点是黑色的；

3、每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存数据；

4、任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；

5、每个节点到达其可达的叶子节点是所有路径，都包含相同数目的黑色节点。

------

红黑树的添加  查找  删除节点：

删除：

①  叶子节点直接删除

②  只有一个子节点的用子节点代替

③  有两个子节点的，用前驱或后继节点代替( ② 的推广描述)



红黑树的平衡变换

1. 变色 黑变红 红变黑

2. 左旋  调整三个指针

   （1）断开三个指针

      (2)  下面节点连接到树上

      (3)  下面节点左子树作为上面节点的右子树

      (4)  上面节点作为下面节点左子树

3. 右旋  调整三个指针

   （1）断开三个指针

      (2)  下面节点连接到树上

      (3)  下面节点右子树作为上面节点的左子树

      (4)  上面节点作为下面节点右子树

------

变色和旋转规则：

​	**新插入节点默认都是红色==重要前提==**

​	**最开始调整的时候是将插入结点作为当前节点。**

1.  变色的情况：

   条件：父节点和叔叔节点都是红色-==红色相连且叔叔红色==

   （1）将父节点和叔叔节点设为黑色。

   （2）将祖父节点设为红色。`根节点除外`

   （3）将祖父节点作为新的当前节点。

2.  左旋的情况

   条件：①父节点是红色；②叔叔节点是黑色；③且当前节点是右子树

   父节点左旋

3. 右旋的情况

   条件：①父节点是红色；②叔叔节点是黑色；③且当前节点是左子树

   注意：右旋的时候需要变色   

   举例：下面的红黑树插入数据：6  变色  左旋  右旋都涉及到了

   ![image-20200904125410493](C:\Users\hds\AppData\Roaming\Typora\typora-user-images\image-20200904125410493.png)

##  红黑树 全链路讲解

https://www.bilibili.com/video/BV135411h7wJ?spm_id_from=333.788.b_636f6d6d656e74.15

补充前驱、后继节点相关：https://www.cnblogs.com/xiejunzhao/p/f5f362c1a89da1663850df9fc4b80214.html



##  B+树

#  5. 分布式

##  5.1  双写一致性保证 ~~未看~~